{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Monitor the Environment in Azure Machine Learning (Data Drift)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Verify the azureml-datadrift package is installed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip show azureml-datadrift"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Connect to your workspace"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1621970050567
        }
      },
      "outputs": [],
      "source": [
        "from azureml.core import Workspace\n",
        "\n",
        "# Load the workspace from the saved config file\n",
        "ws = Workspace.from_config()\n",
        "print('Ready to work with', ws.name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Create a baseline dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1621970061564
        }
      },
      "outputs": [],
      "source": [
        "from azureml.core import Datastore, Dataset\n",
        "\n",
        "\n",
        "# Upload the baseline data\n",
        "default_ds = ws.get_default_datastore()\n",
        "default_ds.upload_files(files=['./data/diabetes.csv', './data/diabetes2.csv'],\n",
        "                       target_path='diabetes-baseline',\n",
        "                       overwrite=True, \n",
        "                       show_progress=True)\n",
        "\n",
        "# Create and register the baseline dataset\n",
        "print('Registering baseline dataset...')\n",
        "baseline_data_set = Dataset.Tabular.from_delimited_files(path=(default_ds, 'diabetes-baseline/*.csv'))\n",
        "baseline_data_set = baseline_data_set.register(workspace=ws, \n",
        "                           name='diabetes baseline',\n",
        "                           description='diabetes baseline data',\n",
        "                           tags = {'format':'CSV'},\n",
        "                           create_new_version=True)\n",
        "\n",
        "print('Baseline dataset registered!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Create a target dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1621970078472
        }
      },
      "outputs": [],
      "source": [
        "import datetime as dt\n",
        "import pandas as pd\n",
        "\n",
        "print('Generating simulated data...')\n",
        "\n",
        "# Load the smaller of the two data files\n",
        "data = pd.read_csv('data/diabetes2.csv')\n",
        "\n",
        "# We'll generate data for the past 6 weeks\n",
        "weeknos = reversed(range(6))\n",
        "\n",
        "file_paths = []\n",
        "for weekno in weeknos:\n",
        "    \n",
        "    # Get the date X weeks ago\n",
        "    data_date = dt.date.today() - dt.timedelta(weeks=weekno)\n",
        "    \n",
        "    # Modify data to ceate some drift\n",
        "    data['Pregnancies'] = data['Pregnancies'] + 1\n",
        "    data['Age'] = round(data['Age'] * 1.2).astype(int)\n",
        "    data['BMI'] = data['BMI'] * 1.1\n",
        "    \n",
        "    # Save the file with the date encoded in the filename\n",
        "    file_path = 'data/diabetes_{}.csv'.format(data_date.strftime(\"%Y-%m-%d\"))\n",
        "    data.to_csv(file_path)\n",
        "    file_paths.append(file_path)\n",
        "\n",
        "# Upload the files\n",
        "path_on_datastore = 'diabetes-target'\n",
        "default_ds.upload_files(files=file_paths,\n",
        "                       target_path=path_on_datastore,\n",
        "                       overwrite=True,\n",
        "                       show_progress=True)\n",
        "\n",
        "# Use the folder partition format to define a dataset with a 'date' timestamp column\n",
        "partition_format = path_on_datastore + '/diabetes_{date:yyyy-MM-dd}.csv'\n",
        "target_data_set = Dataset.Tabular.from_delimited_files(path=(default_ds, path_on_datastore + '/*.csv'),\n",
        "                                                       partition_format=partition_format)\n",
        "\n",
        "# Register the target dataset\n",
        "print('Registering target dataset...')\n",
        "target_data_set = target_data_set.with_timestamp_columns('date').register(workspace=ws,\n",
        "                                                                          name='diabetes target',\n",
        "                                                                          description='diabetes target data',\n",
        "                                                                          tags = {'format':'CSV'},\n",
        "                                                                          create_new_version=True)\n",
        "\n",
        "print('Target dataset registered!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Assign the compute target"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1621970182302
        }
      },
      "outputs": [],
      "source": [
        "from azureml.core.compute import ComputeTarget, AmlCompute\n",
        "from azureml.core.compute_target import ComputeTargetException\n",
        "\n",
        "cluster_name = \"ac-aml-cluster\"\n",
        "\n",
        "try:\n",
        "    # Check for existing compute target\n",
        "    training_cluster = ComputeTarget(workspace=ws, name=cluster_name)\n",
        "    print('Found existing cluster, use it.')\n",
        "except ComputeTargetException:\n",
        "    # If it doesn't already exist, say so\n",
        "    print('There is no existing cluster by that name.')\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Create the data drift monitor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1621970213957
        }
      },
      "outputs": [],
      "source": [
        "from azureml.datadrift import DataDriftDetector\n",
        "\n",
        "# set up feature list\n",
        "features = ['Pregnancies', 'Age', 'BMI']\n",
        "\n",
        "# set up data drift detector\n",
        "monitor = DataDriftDetector.create_from_datasets(ws, 'mslearn-diabates-drift', baseline_data_set, target_data_set,\n",
        "                                                      compute_target=cluster_name, \n",
        "                                                      frequency='Week', \n",
        "                                                      feature_list=features, \n",
        "                                                      drift_threshold=.3, \n",
        "                                                      latency=24)\n",
        "monitor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Backfill the data drift monitor\n",
        "\n",
        "**Note:** This may take some time to run."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1621971219146
        }
      },
      "outputs": [],
      "source": [
        "from azureml.widgets import RunDetails\n",
        "\n",
        "backfill = monitor.backfill(dt.datetime.now() - dt.timedelta(weeks=6), dt.datetime.now())\n",
        "\n",
        "RunDetails(backfill).show()\n",
        "backfill.wait_for_completion()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Analyze data drift"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1621971233332
        }
      },
      "outputs": [],
      "source": [
        "drift_metrics = backfill.get_metrics()\n",
        "for metric in drift_metrics:\n",
        "    print(metric, drift_metrics[metric])"
      ]
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "python3-azureml"
    },
    "kernelspec": {
      "display_name": "Python 3.6 - AzureML",
      "language": "python",
      "name": "python3-azureml"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
